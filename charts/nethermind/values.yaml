# Default values for nethermind.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

# -- can be Energy Web,Exosama,Goerli (testnet),Gnosis, Chiado ,Holesky (testnet),Mainnet,Sepolia (testnet),Volta (testnet)
network: sepolia

# -- Path to store data
data:
  path: /nethermind-data

# -- JWT secret used by client as a secret. Change this value.
jwt: ecb22bc24e7d4061f7ed690ccd5846d7d73f5d2b9733267e12f56790398d908a

# -- Log level for the node
logLevel: 'INFO'

# -- syncMode can be FastSync, SnapSync
syncMode: 'SnapSync'

jsonrpc:
  enabled: true
  modules:
    - Eth
    - Subscribe
    - Trace
    - TxPool
    - Web3
    - Personal
    - Proof
    - Net
    - Parity
    - Health

  engine:
    modules:
      - Net
      - Eth
      - Subscribe
      - Engine
      - Web3
      - Client
image:
  repository: nethermind/nethermind
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  ports:
    httprpc:
      port: 8545
      protocol: TCP
    ws:
      port: 8546
      protocol: TCP
    auth:
      port: 8551
      protocol: TCP
    p2ptcp:
      port: 30303
      protocol: TCP
    p2pudp:
      port: 30303
      protocol: UDP
    metrics:
      port: 9545
      protocol: TCP

# @default -- See `values.yaml`
livenessProbe:
  tcpSocket:
    port: httprpc
  initialDelaySeconds: 60
  periodSeconds: 120

# @default -- See `values.yaml`
readinessProbe:
  tcpSocket:
    port: httprpc
  initialDelaySeconds: 10
  periodSeconds: 10

ingress:
  enabled: true
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

persistence:
  # -- Uses an EmptyDir when not enabled
  enabled: false
  # -- Use an existing PVC when persistence.enabled
  existingClaim: ""
  # -- Access mode for the volume claim template
  accessModes:
  - ReadWriteOnce
  # -- Requested size for volume claim template
  size: 20Gi
  # -- Use a specific storage class
  # E.g 'local-path' for local storage to achieve best performance
  # Read more (https://github.com/rancher/local-path-provisioner)
  storageClassName: ""
  # -- Annotations for volume claim template
  annotations: {}
  # -- Selector for volume claim template
  selector: {}
  #   matchLabels:
  #     app.kubernetes.io/name: something

metrics:
  enabled: true
  # -- add node to ethstat
  ethStats: true

healthChecks:
  enabled: true
  ui:
    enabled: true

serviceMonitor:
  # -- If true, a ServiceMonitor CRD is created for a prometheus operator
  # https://github.com/coreos/prometheus-operator
  enabled: false
  # -- Path to scrape
  path: /debug/metrics
  # -- Alternative namespace for ServiceMonitor
  namespace: ""
  # -- Additional ServiceMonitor labels
  labels: {}
  # -- Additional ServiceMonitor annotations
  annotations: {}
  # -- ServiceMonitor scrape interval
  interval: 1m
  # -- ServiceMonitor scheme
  scheme: http
  # -- ServiceMonitor TLS configuration
  tlsConfig: {}
  # -- ServiceMonitor scrape timeout
  scrapeTimeout: 30s
  # -- ServiceMonitor relabelings
  relabelings: []

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}
